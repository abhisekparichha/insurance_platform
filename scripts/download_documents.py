#!/usr/bin/env python3
"""Download pending product documents with throttling and retries."""
from __future__ import annotations

import argparse
import asyncio
import json
import logging
import random
import sqlite3
import time
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Iterable, List, Optional

from src.database import InsuranceRepository
from src.document_processing import DocumentProcessor, DocumentProcessorConfig
from src.models import ProductDocument

LOGGER = logging.getLogger("download_documents")


@dataclass
class DownloadJob:
    """Serializable metadata for a document download."""

    document_id: str
    insurer_id: str
    insurer_name: str
    document_type: str
    source_url: str
    product_id: Optional[str]
    product_name: str
    metadata: Dict[str, str]

    def to_product_document(self) -> ProductDocument:
        """Convert into the domain model expected by DocumentProcessor."""
        return ProductDocument(
            document_id=self.document_id,
            insurer_id=self.insurer_id,
            document_type=self.document_type,
            source_url=self.source_url,
            product_id=self.product_id or None,
            metadata=dict(self.metadata),
        )


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Download product documents recorded by the ingestion pipeline.",
    )
    parser.add_argument(
        "--db",
        type=Path,
        default=Path("data") / "insurance.db",
        help="Path to the SQLite database populated by the pipeline.",
    )
    parser.add_argument(
        "--documents-dir",
        type=Path,
        default=Path("data") / "documents",
        help="Directory where downloaded files will be written.",
    )
    parser.add_argument(
        "--queue",
        type=Path,
        default=Path("data") / "document_download_queue.json",
        help="Optional manifest generated by the pipeline; updated after each run.",
    )
    parser.add_argument(
        "--min-delay",
        type=float,
        default=1.5,
        help="Minimum seconds to wait between requests to the same insurer.",
    )
    parser.add_argument(
        "--max-delay",
        type=float,
        default=4.0,
        help="Maximum seconds to wait between requests to the same insurer.",
    )
    parser.add_argument(
        "--max-parallel-insurers",
        type=int,
        default=2,
        help="How many insurers to download from concurrently.",
    )
    parser.add_argument(
        "--retries",
        type=int,
        default=2,
        help="How many additional passes to attempt for failed downloads.",
    )
    parser.add_argument(
        "--request-timeout",
        type=int,
        default=30,
        help="Per-request timeout passed to the underlying DocumentProcessor.",
    )
    return parser.parse_args()


def load_jobs(queue_path: Path, db_path: Path) -> List[DownloadJob]:
    if queue_path.exists():
        LOGGER.info("Loading pending downloads from %s", queue_path)
        return load_jobs_from_queue(queue_path)
    LOGGER.info("Queue file not found; reading pending documents from %s", db_path)
    return load_jobs_from_db(db_path)


def load_jobs_from_queue(queue_path: Path) -> List[DownloadJob]:
    payload = json.loads(queue_path.read_text(encoding="utf-8"))
    jobs = []
    for entry in payload.get("documents", []):
        jobs.append(
            DownloadJob(
                document_id=entry["document_id"],
                insurer_id=entry["insurer_id"],
                insurer_name=entry.get("insurer_name", entry["insurer_id"]),
                document_type=entry["document_type"],
                source_url=entry["source_url"],
                product_id=entry.get("product_id") or None,
                product_name=entry.get("product_name", ""),
                metadata={
                    "page_url": entry.get("page_url", ""),
                    "anchor_text": entry.get("anchor_text", ""),
                    "discovery_method": entry.get("discovery_method", ""),
                    "extension": entry.get("extension", ""),
                },
            )
        )
    return jobs


def load_jobs_from_db(db_path: Path) -> List[DownloadJob]:
    connection = sqlite3.connect(db_path)
    connection.row_factory = sqlite3.Row
    cursor = connection.cursor()
    query = """
        SELECT
            d.document_id,
            d.insurer_id,
            i.name AS insurer_name,
            d.product_id,
            p.name AS product_name,
            d.document_type,
            d.source_url,
            d.local_path,
            d.metadata_json
        FROM product_documents d
        JOIN insurers i ON i.insurer_id = d.insurer_id
        LEFT JOIN products p ON p.product_id = d.product_id
    """
    cursor.execute(query)
    jobs: List[DownloadJob] = []
    for row in cursor.fetchall():
        source_url = row["source_url"]
        if not source_url:
            continue
        local_path = row["local_path"]
        if local_path:
            path_obj = Path(local_path)
            if not path_obj.is_absolute():
                path_obj = Path(local_path)
            if path_obj.exists():
                continue
        metadata = {}
        metadata_json = row["metadata_json"]
        if metadata_json:
            try:
                metadata = json.loads(metadata_json)
            except json.JSONDecodeError:
                metadata = {}
        jobs.append(
            DownloadJob(
                document_id=row["document_id"],
                insurer_id=row["insurer_id"],
                insurer_name=row["insurer_name"],
                document_type=row["document_type"],
                source_url=source_url,
                product_id=row["product_id"],
                product_name=row["product_name"] or "",
                metadata={
                    "page_url": metadata.get("page_url", ""),
                    "anchor_text": metadata.get("anchor_text", ""),
                    "discovery_method": metadata.get("discovery_method", ""),
                    "extension": metadata.get("extension", ""),
                },
            )
        )
    cursor.close()
    connection.close()
    return jobs


def group_jobs_by_insurer(jobs: Iterable[DownloadJob]) -> Dict[str, List[DownloadJob]]:
    grouped: Dict[str, List[DownloadJob]] = defaultdict(list)
    for job in jobs:
        grouped[job.insurer_id].append(job)
    return grouped


def process_insurer_sync(
    insurer_id: str,
    insurer_name: str,
    jobs: List[DownloadJob],
    config: DocumentProcessorConfig,
    min_delay: float,
    max_delay: float,
) -> tuple[List[ProductDocument], List[DownloadJob]]:
    processor = DocumentProcessor(config)
    successes: List[ProductDocument] = []
    failures: List[DownloadJob] = []
    for job in jobs:
        delay = random.uniform(min_delay, max_delay) if max_delay > 0 else 0.0
        if delay > 0:
            time.sleep(delay)
        document = job.to_product_document()
        try:
            processed = processor.process_document(document)
            if processed.local_path:
                LOGGER.info(
                    "[%s] Downloaded %s (%s)",
                    insurer_name,
                    processed.document_id,
                    processed.document_type,
                )
                successes.append(processed)
            else:
                LOGGER.warning(
                    "[%s] Download skipped for %s (no local path)",
                    insurer_name,
                    processed.document_id,
                )
                failures.append(job)
        except Exception as exc:  # pylint: disable=broad-except
            LOGGER.warning(
                "[%s] Failed to download %s (%s): %s",
                insurer_name,
                job.document_id,
                job.source_url,
                exc,
            )
            failures.append(job)
    return successes, failures


async def run_round(
    pending: Dict[str, List[DownloadJob]],
    config: DocumentProcessorConfig,
    min_delay: float,
    max_delay: float,
    max_parallel: int,
) -> tuple[List[ProductDocument], Dict[str, List[DownloadJob]]]:
    loop = asyncio.get_running_loop()
    semaphore = asyncio.Semaphore(max(1, max_parallel))
    tasks = []

    for insurer_id, jobs in pending.items():
        insurer_name = jobs[0].insurer_name if jobs else insurer_id

        async def run_for_insurer(
            iid: str = insurer_id,
            iname: str = insurer_name,
            ijobs: List[DownloadJob] = jobs,
        ) -> tuple[str, List[ProductDocument], List[DownloadJob]]:
            async with semaphore:
                successes, failures = await loop.run_in_executor(
                    None,
                    process_insurer_sync,
                    iid,
                    iname,
                    ijobs,
                    config,
                    min_delay,
                    max_delay,
                )
                return iid, successes, failures

        tasks.append(asyncio.create_task(run_for_insurer()))

    results = await asyncio.gather(*tasks)
    successes: List[ProductDocument] = []
    failed: Dict[str, List[DownloadJob]] = defaultdict(list)
    for insurer_id, insurer_successes, insurer_failures in results:
        successes.extend(insurer_successes)
        if insurer_failures:
            failed[insurer_id].extend(insurer_failures)
    return successes, failed


def flatten_jobs(pending: Dict[str, List[DownloadJob]]) -> List[DownloadJob]:
    items: List[DownloadJob] = []
    for jobs in pending.values():
        items.extend(jobs)
    return items


def write_queue(queue_path: Path, jobs: Iterable[DownloadJob]) -> None:
    documents = [
        {
            "insurer_id": job.insurer_id,
            "insurer_name": job.insurer_name,
            "document_id": job.document_id,
            "document_type": job.document_type,
            "product_id": job.product_id or "",
            "product_name": job.product_name,
            "source_url": job.source_url,
            "page_url": job.metadata.get("page_url", ""),
            "anchor_text": job.metadata.get("anchor_text", ""),
            "discovery_method": job.metadata.get("discovery_method", ""),
            "extension": job.metadata.get("extension", ""),
        }
        for job in jobs
    ]
    payload = {
        "generated_at": datetime.now(timezone.utc).isoformat(timespec="seconds"),
        "document_count": len(documents),
        "documents": documents,
    }
    queue_path.parent.mkdir(parents=True, exist_ok=True)
    queue_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")


async def download_all(args: argparse.Namespace, jobs: List[DownloadJob]) -> None:
    if args.max_delay < args.min_delay:
        raise ValueError("max-delay must be greater than or equal to min-delay.")

    grouped = group_jobs_by_insurer(jobs)
    if not grouped:
        LOGGER.info("No documents require downloading.")
        write_queue(args.queue, [])
        return

    processor_config = DocumentProcessorConfig(
        base_dir=args.documents_dir,
        download_documents=True,
        request_timeout=args.request_timeout,
    )

    repository = InsuranceRepository(args.db)
    try:
        for attempt in range(1, args.retries + 2):
            if not grouped:
                break
            total_docs = sum(len(batch) for batch in grouped.values())
            LOGGER.info(
                "Download pass %d/%d: %d insurers, %d documents",
                attempt,
                args.retries + 1,
                len(grouped),
                total_docs,
            )
            successes, failures = await run_round(
                grouped,
                processor_config,
                args.min_delay,
                args.max_delay,
                args.max_parallel_insurers,
            )
            if successes:
                repository.upsert_documents(successes)
            grouped = failures
            if grouped:
                LOGGER.info("%d documents still pending after pass %d", sum(len(batch) for batch in grouped.values()), attempt)

        if grouped:
            LOGGER.warning(
                "Unable to download %d documents after %d passes; saving queue for retry.",
                sum(len(batch) for batch in grouped.values()),
                args.retries + 1,
            )
            write_queue(args.queue, flatten_jobs(grouped))
        else:
            LOGGER.info("All pending documents downloaded successfully.")
            write_queue(args.queue, [])
    finally:
        repository.close()


def main() -> None:
    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
    args = parse_args()
    jobs = load_jobs(args.queue, args.db)
    if not jobs:
        LOGGER.info("No pending downloads discovered. Nothing to do.")
        write_queue(args.queue, [])
        return
    args.documents_dir.mkdir(parents=True, exist_ok=True)
    asyncio.run(download_all(args, jobs))


if __name__ == "__main__":
    main()
